---
title: "Homework 4"
author: "Ankita Pattnaik"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

```{r, message= FALSE}
library(tidymodels)
library(tidyverse)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(klaR) 
library(pROC)
library(readr)
tidymodels_prefer()
```
```{r}
titanic<- read.csv('/Users/ankitapattnaik/Downloads/homework-3/data/titanic.csv', stringsAsFactor=T)
```



Create a recipe for this dataset identical to the recipe you used in Homework 3.

1.Split the data, stratifying on the outcome variable, survived. You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations.

```{r}
set.seed(3435)

titanic_split <- initial_split(titanic, prop = 0.70,
                                strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

dim(titanic_train)
dim(titanic_test)

titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + 
                           parch + fare, data = titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ starts_with("sex"):fare) %>%
  step_interact( ~ age:fare) 

```

Because we are looking at the data of survived, we can draw conclusions based off a sample of the total population versus the whole population itself. Due to the high number of observations, a sample set of the observations might lead to more accurate predictions.




2. Fold the training data. Use k-fold cross-validation, with k=10.

```{r}
titanic_fold <- vfold_cv(titanic_train, v = 10)
titanic_fold
```




3. In your own words, explain what we are doing in Question 2. What is k-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we did use the entire training set, what resampling method would that be?

In question 2 we are finding a value of best fit that would help yield more accurate degree for a polynomial regression model. By using it we get a more accurate fit, leading to more accurate conclusions on the data. We go through the already split data 10 times, so we can find the best fit out of the 10. 





4. Set up workflows for 3 models:

A logistic regression with the glm engine;
A linear discriminant analysis with the MASS engine;
A quadratic discriminant analysis with the MASS engine.

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you’ll fit to each fold.

There are three models and 10 fold so 3 x 10 = 30. So in total there would be 30 folds.



5. Fit each of the models created in Question 4 to the folded data.

```{r}
degree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)
degree_grid

tune_res_log <- tune_grid(object = log_wkflow, 
  resamples = titanic_fold, 
  grid = degree_grid)

tune_res_lda <- tune_grid(object = lda_wkflow, 
  resamples = titanic_fold, 
  grid = degree_grid)

tune_res_qda <- tune_grid(object = qda_wkflow, 
  resamples = titanic_fold, 
  grid = degree_grid)
```





6.Use collect_metrics() to print the mean and standard errors of the performance metric accuracy across all folds for each of the four models.

```{r}
collect_metrics(tune_res_log)
collect_metrics(tune_res_lda)
collect_metrics(tune_res_qda)
```

Decide which of the 3 fitted models has performed the best. Explain why. (Note: You should consider both the mean accuracy and its standard error.)

Of the three, we should use the logistic regression model because it has the highest accuracy with 0.81 and 0.83 for the mean in accuracy and roc_auc.  




7. Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).
```{r}
log_fit <- fit(log_wkflow, titanic_train)
```



8. Finally, with your fitted model, use predict(), bind_cols(), and accuracy() to assess your model’s performance on the testing data!
```{r}
titanic_predict <- predict(log_fit, new_data = titanic_train, type = "prob")
titanic_bind <- bind_cols(titanic_predict, titanic_train)
titanic_augment <- augment(log_fit, new_data= titanic_train) %>% accuracy(truth=survived, estimate=.pred_class)
titanic_augment
```
Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

They are very close to each other, except testing accuracy holds higher accuracy than average accuracy.